# LLM_Code： A Multilingual Instruction Dataset on Code and trained on large language models.
[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE) 
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

This is the repository for the `ID_Code`project, which aims to build a multilingual instruction dataset on Code tasks. 

## Overview
As far as we know, at present, the dataset on instruction-tuning's code tasks is relatively messy, single-language,single-programming language, and the variety of tasks covered by the dataset is not wide enough. On the other hand, there are few open sourse datasets for instruction tuning in code tasks.

For this end, we propose this project, with the following advantages:
- 1. Multilingual Dataset: Include code samples from multiple programming languages(Java, Python, C, C#, Go, PHP, JavaScript, Ruby) to create a multilingual dataset. At the same time, this dataset also contains code examples in Chinese and English languages. This allows the model to learn instructions in different programming language contexts, making it more versatile.
- 2. Task diversity: Expand the dataset to cover a wide range of code trasks, including code summarization, code generation, code search.. This ensures that the instructions can handle different types of code tasks. And include a variety of code tasks with varying complexities and requirements. In addition, involve tasks of different levels, such as beginner, intermediate, and advanced, to cover a broad spectrum of programming skills and knowledge.
- 3. Multi-programming paradigms: Include code examples that cover different programming paradigms such as procedural, object-oriented, functional, or event-driven programming. This will provide a wider range of code tasks for the instruction-tuning model to learn from and generate instructions for.
- 4. Real-world code examples:  Include code snippets or excerpts from real-world projects to provide more realistic and practical code tasks. This helps the instruction-tuning model generate instructions that are applicable to real-world scenarios.
- 5. Quality assurance: Ensure the dataset has accurate and high-quality instructions for each code task.

The repository contains the following:
- The `MID_Dataset` used for fine-tuning the model
- The code for fine-tuning the model
- Model weight
- The code for evaluation

## Dataset release
[`data/MID_all_data.json`]() contains xx instruction-following dataused for fine-tuning the MID Code alpace model.
This file is a list of dictionaries, each dictionary contains the following fileds:
- `instruction`: describes the task that the model should perform. 
- `input`: optional code or context for the task. For example, if the instruction is 'Please summarize this PHP code.', the input is the PHP code.
- `output`: the answer to the instruction. 

All data in our collection is formatted into the same templates, where each sample is as follows:
```
[
{"instruction":  `string`,
"input":  `string`, # (may be empty)
"output": `string`}
]
```

Due to the different code tasks, we choose which filed to generate with  `gpt-3.5-turbo` or human. Unlike `self-struct` technology to generate data, most of the code in our data comes from the real world, whereas most instruction choices are generated by `gpt-3.5-turbo`. The detailed process of data processing is described in the next section.

## Dataset Collection & Processing


## Finetuning

## Inference

## Open-Sourced Datasets
| Dataset                                                                                           | Release time | Scale        | Lang  | Programming Lang | Task |
| :---:                                                                                             |    :----:    |        :---: |  :---:|         :---:    |:---: |
| [Instruct-to-Code](https://huggingface.co/datasets/Graverman/Instruct-to-Code)                    | Mar 28,2023  | 451k         | Mul   |   python…et al.  |et al.| 
| [godot_dodo_4x_60k](https://github.com/minosvasilias/godot-dodo/tree/main/data/godot_dodo_4x_60k) |  Apr 27,2023 |    62533     | EN    |    GDScript      |Code Generation|
|[TSSB-3M-instructions ](https://huggingface.co/datasets/zirui3/TSSB-3M-instructions)               | Apr 28,2023  | 3M           | EN    |python…et al.     |Code bugfix|
|[Codegen](https://github.com/teknium1/GPTeacher/tree/main/Codegen)                                 | May 4,2023   |4535          | EN    |C++,Node.js,Python,shell script,Java,JavaScript,et al.| Code Generation,Code Summary,QA et al.|
|[codealpaca](https://github.com/sahil280114/codealpaca/tree/master)                                | May 13,2023  |20k           | EN    |  HTML,CSS,Java,SQL,Python,JavaScript,JSX,C++,Swift,Ruby,PHP,et al.      | Code Generation,Code Search et al.|
|[CodeGPT](https://github.com/zxx000728/CodeGPT)                                                    |  May 10,2023 |32k           | CN    |C#,C,C++,Go,Java,JavaScript,PHP,Python,Ruby,et al.|Code Generation,Code Search, QA el al.|


## Citation
